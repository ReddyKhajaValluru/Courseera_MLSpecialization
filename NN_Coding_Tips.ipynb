{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMX4/RJ/nAGXWF0lpnu1W/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReddyKhajaValluru/Courseera_MLSpecialization/blob/main/NN_Coding_Tips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Imports"
      ],
      "metadata": {
        "id": "9kIcoejvE4-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "\n",
        "# for preparing data and error metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "eKTrWHZKFCkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models."
      ],
      "metadata": {
        "id": "5UWlV9j7y5f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into training, cross validation, and test sets\n",
        "\n",
        "In previous labs, you might have used the entire dataset to train your models. In practice however, it is best to hold out a portion of your data to measure how well your model generalizes to new examples. This will let you know if the model has overfit to your training set.\n",
        "\n",
        "As mentioned in the lecture, it is common to split your data into three parts:\n",
        "\n",
        "* ***training set*** - used to train the model\n",
        "* ***cross validation set (also called validation, development, or dev set)*** - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.\n",
        "* ***test set*** - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.\n",
        "\n",
        "Scikit-learn provides a [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split your data into the parts mentioned above. In the code cell below, you will split the entire dataset into 60% training, 20% cross validation, and 20% test."
      ],
      "metadata": {
        "id": "ufudlidADgAB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BB47blPDXOu"
      },
      "outputs": [],
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data\n",
        "Here we will convert the given input data into the format required to get best output out of Neural network.\n",
        "\n",
        "\n",
        "1.   We can generate more features with the current features (Mostly for regression without neural networks)\n",
        "2.   We can normalize the features if they have widely different ranges of values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pHiO3NAuD1Pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding more polynomial features\n",
        "The code below demonstrates how to do this using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class.\n",
        "\n",
        "PolynomialFeatures will create a new matrix consisting of all polynomial combinations of the features with a degree less than or equal to the degree we just gave the model (i.e degree=2 here).\n",
        "\n",
        "\n",
        "\n",
        "1.   If the input is univariate [x] output: [1 x x^2]\n",
        "2.   If the input is Multivariate [x1,x2] output: [1 x1 x2 x1^2 x1x2 x2^2]\n",
        "\n",
        "Above mentioned order is also followed. If you don't want the bias term 1, you can simply specify \"include_bias=False\" as an argument."
      ],
      "metadata": {
        "id": "SfaeVbMQIZyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "0havPDTsIPvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features\n",
        "# Instantiate the class to make polynomial features\n",
        "degree = 2\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "# Compute the number of features and transform the training set\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "# Add the polynomial features to the cross validation set and test set\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "X_test_mapped = poly.transform(x_test)"
      ],
      "metadata": {
        "id": "fj37Aj-MHVBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature scaling (Normalizing Data)\n",
        "In the course of ML specialization, you saw that it is usually a good idea to perform feature scaling to help your model converge faster. This is especially true if your input features have widely different ranges of values. For example, if you add polynomial terms, your input features will indeed have different ranges. i.e, $x$ runs from around 1600 to 3600, while $x^2$ will run from 2.56 million to 12.96 million.\n",
        "\n",
        "So, it's good to practice feature scaling. For that, you will use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class from scikit-learn. This computes the z-score of your inputs. As a refresher, the z-score is given by the equation:\n",
        "\n",
        "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "where $\\mu$ is the mean of the feature values and $\\sigma$ is the standard deviation. The same $\\mu$ and $\\sigma$ should be used for the CV and test set, before sending to the neural network."
      ],
      "metadata": {
        "id": "QcuTJmIkIZLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "1ERJPkDEISdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features using the z-score\n",
        "# Instantiate the class\n",
        "scaler = StandardScaler()\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
        "# Scale the cross validation set and test set using the mean and standard deviation of the training set\n",
        "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
        "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
      ],
      "metadata": {
        "id": "MVjRRULfHT_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Model (with scikit-learn)\n",
        "Now, you will create and train a regression model. As we have all the polynomial features we can simply use linear regression for all 4 tasks - univariate and multivariate linear and polynomial regressions.\n",
        "\n",
        "For this lab, you will use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class but take note that there are other [linear regressors](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors) which you can also use."
      ],
      "metadata": {
        "id": "75WrPMjZSeQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mapped_scaled, y_train )\n",
        "\n",
        "# Compute the training error(MSE)\n",
        "yhat = model.predict(X_train_mapped_scaled)\n",
        "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\") # MSE dont have 2 in the formulae so we use 2."
      ],
      "metadata": {
        "id": "CCOzv37ISk60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Model (with Neural Networks)"
      ],
      "metadata": {
        "id": "2isGJ4vvVqha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Defining Model"
      ],
      "metadata": {
        "id": "vIlF6wVsWVxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(units=25, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name = 'layer1'),\n",
        "        Dense(units=15, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name = 'layer2'),\n",
        "        Dense(units=1, activation = 'linear', kernel_regularizer=tf.keras.regularizers.l1(0.1), name = 'layer3') # we can use relu also if the output is non negative\n",
        "    ], name = \"my_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "M2fcTAI1VyWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to instantiate the weights and bias parameters at this point.  This is useful when exploring Tensorflow models using `model.summary()` immediately after defining the model. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.\n",
        "\n",
        "The above can be used if you want to specify additional properties about the input layer. Instead, if you dont have any info other that input dimenion (i.e number of input features) we can write the following concise versions\n",
        "\n",
        "1.   Dense(units=25, input_dim=2, activation = 'relu', name = 'layer1')\n",
        "2.   Dense(units=25, input_shape=(2,), activation = 'relu', name = 'layer1')\n",
        "\n",
        "\n",
        "\n",
        ">**Note 2:** Tensorflow gives us an oppurtunity to provide different regularization to different layers.\n",
        "\n",
        "The `model.summary()` provides a description of the network:"
      ],
      "metadata": {
        "id": "OSYyjBehbHoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QvYmuOhVcLCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2: Setup the loss and optimizer"
      ],
      "metadata": {
        "id": "sJMb3LePW1Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.MeanSquaredError(), # simply we can also say loss='mse'\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")"
      ],
      "metadata": {
        "id": "jAPR1XQWZCO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3: Train the model"
      ],
      "metadata": {
        "id": "P_5xY_wiW6nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "nPuhEk7oZErh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Predict output and calculate error\n",
        "\n",
        "When *evaluating* a linear regression model, you average the squared error difference of the predicted values and the target values.\n",
        "\n",
        "$$ J_\\text{test}(\\mathbf{w},b) =\n",
        "            \\frac{1}{2m_\\text{test}}\\sum_{i=0}^{m_\\text{test}-1} ( f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}_\\text{test}) - y^{(i)}_\\text{test} )^2\n",
        "            \\tag{1}\n",
        "$$"
      ],
      "metadata": {
        "id": "UiMnhsL1W9xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the cross validation MSEs\n",
        "yhat = model.predict(X_cv)\n",
        "cv_mse = mean_squared_error(y_cv, yhat) / 2"
      ],
      "metadata": {
        "id": "sMYbSbsVZ-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Classification (Logistic Regression)"
      ],
      "metadata": {
        "id": "3jDIHQw2ga3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Defining Model"
      ],
      "metadata": {
        "id": "CHWUl5jshUwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(units=25, input_dim=2, activation = 'relu'),\n",
        "        Dense(units=15, activation = 'relu'),\n",
        "        Dense(units=1, activation = 'sigmoid')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Kv1xZ_5ZhUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2: Setup the loss and optimizer"
      ],
      "metadata": {
        "id": "1HtMq0hyhUwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
        ")"
      ],
      "metadata": {
        "id": "o6JDItx1hUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3: Train the model"
      ],
      "metadata": {
        "id": "woCD1mYHhUwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "DSplcNcYhUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Predict output and calculate error\n",
        "\n",
        "In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. For example, if your model made wrong predictions for 2 samples out of 5, then you will report an error of `40%` or `0.4`. The code below demonstrates this using  Numpy's [`mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) function.\n",
        "\n",
        "The evaluation function for categorical models used here is simply the fraction of incorrect predictions:  \n",
        "$$ J_{cv} =\\frac{1}{m}\\sum_{i=0}^{m-1}\n",
        "\\begin{cases}\n",
        "    1, & \\text{if $\\hat{y}^{(i)} \\neq y^{(i)}$}\\\\\n",
        "    0, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "1bd7enMkhUwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the output\n",
        "yhat = model.predict(X_cv)\n",
        "# Set the threshold for classification\n",
        "threshold = 0.5\n",
        "yhat = np.where(yhat >= threshold, 1, 0)\n",
        "# Record the cross validation errors\n",
        "cv_mse = np.mean(yhat != y_cv)"
      ],
      "metadata": {
        "id": "R994uzyxhUwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use accuracy score from scikit-learn library for evaluating the performance of classification models.\n",
        "\n",
        "Remember error is 1-accuracy"
      ],
      "metadata": {
        "id": "4l8Hu0IdSv2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Example true labels (ground truth)\n",
        "y_true = np.array([0, 1, 1, 0, 1, 0])\n",
        "\n",
        "# Example predicted labels from a classification model\n",
        "y_pred = np.array([0, 1, 0, 0, 1, 1])\n",
        "\n",
        "# Compute accuracy using accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"True Labels:\", y_true)\n",
        "print(\"Predicted Labels:\", y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "bI5hlneYTM3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Class Classification (Softmax Regression)"
      ],
      "metadata": {
        "id": "dLbdkCp3lAbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Defining Model"
      ],
      "metadata": {
        "id": "R5LCD_HVlAbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non preferred option: output layer activation = 'softmax' , loss = SparseCategoricalCrossentropy()\n",
        "\n",
        "Preferred option: output layer activation = 'linear' , loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "The preferred option is numerically stable, but remember that the output of the model should be passed through a softmax function to get the probabilities."
      ],
      "metadata": {
        "id": "61sgOQ0NnaBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(units=25, input_dim=2, activation = 'relu'),\n",
        "        Dense(units=15, activation = 'relu'),\n",
        "        Dense(units=9, activation = 'linear')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "G01TFyCqlAbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2: Setup the loss and optimizer"
      ],
      "metadata": {
        "id": "-_bkitSnlAbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
        ")"
      ],
      "metadata": {
        "id": "52-QLWKNlAbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparseCategorialCrossentropy or CategoricalCrossEntropy**\n",
        "\n",
        "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
        "- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.\n",
        "- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded (one-hot vector) where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\n"
      ],
      "metadata": {
        "id": "vSAUdzkhpRef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3: Train the model"
      ],
      "metadata": {
        "id": "gGLF9HXwlAbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "lL4oWYLhlAbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Predict output and calculate error\n",
        "\n",
        "Historically output of linear layer is called logits. The output class is decided by checking the output probabilities (which ever is higher). But there is no need to pass the output through softmax because the argmax can be found using logits also.\n",
        "\n",
        "The error is same as binary classification, since this is also a classification."
      ],
      "metadata": {
        "id": "6x4nDY4RlAbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the output\n",
        "logits = model.predict(X_cv)\n",
        "yhat = tf.nn.softmax(logits).numpy()\n",
        "# Predict the output as the one with highest probability\n",
        "yhat = np.argmax(yhat,axis=1) # axis is written because each row belongs to different input and we need the argmax value in each row\n",
        "# Record the cross validation errors\n",
        "cv_mse = np.mean(yhat != y_cv)"
      ],
      "metadata": {
        "id": "KYvWWQsQlAbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label Classification\n",
        "This is when you have multiple labels in a single image. We can tackle this problem as multiple logistic regressions in a single problem by giving multiple neurons in the output layer."
      ],
      "metadata": {
        "id": "JOJ6NTAmxPyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Defining Model"
      ],
      "metadata": {
        "id": "LkG1fJLrxryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(units=25, input_dim=2, activation = 'relu'),\n",
        "        Dense(units=15, activation = 'relu'),\n",
        "        Dense(units=3, activation = 'sigmoid')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "k7c4xFf2xryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step2: Setup the loss and optimizer"
      ],
      "metadata": {
        "id": "qrlwVL3fxryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(0.01),\n",
        ")"
      ],
      "metadata": {
        "id": "HIL0E9sgxryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3: Train the model"
      ],
      "metadata": {
        "id": "32jQOED5xryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "1Ig1ZrAxxryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step4: Predict output and calculate error\n",
        "\n",
        "The error is same as binary classification, since this is also a classification."
      ],
      "metadata": {
        "id": "xUrBE8s3xryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the output\n",
        "yhat = model.predict(X_cv)\n",
        "# Set the threshold for classification\n",
        "threshold = 0.5\n",
        "yhat = np.where(yhat >= threshold, 1, 0)\n",
        "# Record the cross validation errors\n",
        "cv_mse = np.mean(yhat != y_cv)"
      ],
      "metadata": {
        "id": "q7f3kDhuxryv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}